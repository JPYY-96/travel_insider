# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OFasKJKFSK9OMyh_csqGcSBDMrT50W-a
"""

import pandas as pd
import numpy as np
import joblib
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBRegressor, DMatrix, train
from google.cloud import bigquery
from datetime import datetime
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

# :drawing_pin: Set Up BigQuery Connection
PROJECT_ID = "travel-insider-452211"
DATASET_NAME = "travel_insider_dataset"
TABLE_NAME = "filtered_flights"
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/travel-insider-452211-181bd2eba48e.json"
# :drawing_pin: Initialize BigQuery Client
client = bigquery.Client()
# :drawing_pin: Initialize XGBoost Model
xgb_model = XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=8,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    tree_method='hist',   # <- THIS is correct now
    device='cuda'         # <- NEW: required for GPU support
)


# Parameters for querying data
chunk_size = 600000  # Query 1 million rows at a time
offset = 0
total_rows = 16839840    # Total number of rows to process
# Initialize overall batch number
overall_batch_num = 0

def rmspe(y_true, y_pred):
    return np.sqrt(np.mean(((y_true - y_pred) / y_true) ** 2)) * 100

def nmse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred) / np.mean(y_true) ** 2
# :drawing_pin: Loop over chunks of data
while offset < total_rows:
    print(f":hourglass_flowing_sand: [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Querying data chunk {offset} to {offset + chunk_size}...")
    # Modify query to fetch a chunk of data
    query = f"""
    SELECT *
    FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`
    ORDER BY legId
    LIMIT {chunk_size} OFFSET {offset}
    """
    chunk_data = client.query(query).to_dataframe()
    if chunk_data.empty:
        break
    # :drawing_pin: Feature Engineering for the chunk
    chunk_data["searchDate"] = pd.to_datetime(chunk_data["searchDate"])
    chunk_data["flightDate"] = pd.to_datetime(chunk_data["flightDate"])
    chunk_data["days_to_flight"] = (chunk_data["flightDate"] - chunk_data["searchDate"]).dt.days
    chunk_data["day_of_week"] = chunk_data["flightDate"].dt.dayofweek
    chunk_data["is_weekend"] = (chunk_data["day_of_week"] >= 5).astype(int)
    chunk_data["is_holiday_season"] = chunk_data["flightDate"].dt.month.isin([6, 7, 12]).astype(int)
    chunk_data["days_to_flight_squared"] = chunk_data["days_to_flight"] ** 2
    chunk_data["flight_month"] = chunk_data["flightDate"].dt.month
    chunk_data["flight_year"] = chunk_data["flightDate"].dt.year
    chunk_data["search_month"] = chunk_data["searchDate"].dt.month
    chunk_data["search_day"] = chunk_data["searchDate"].dt.day
    chunk_data["days_to_flight_log"] = np.log1p(chunk_data["days_to_flight"])
    # Remove invalid rows
    chunk_data = chunk_data[chunk_data["days_to_flight"] > 0]
    # :drawing_pin: Convert `isRefundable` to numeric (0 or 1)
    chunk_data['isRefundable'] = chunk_data['isRefundable'].map({'Yes': 1, 'No': 0}).fillna(0).astype(int)
    # One-Hot Encode Categorical Features (Airports & Airlines)
    if overall_batch_num == 0:
        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        # Fit the encoder on the first chunk
        encoded_features = encoder.fit_transform(chunk_data[['startingAirport', 'destinationAirport', 'segmentsAirlineName']])
    else:
        # Transform with the already fitted encoder for subsequent chunks
        encoded_features = encoder.transform(chunk_data[['startingAirport', 'destinationAirport', 'segmentsAirlineName']])
    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out())
    # At this point, we don't yet have the trained feature columns, but we do know the encoded columns
    feature_columns = ['days_to_flight', 'days_to_flight_squared', 'day_of_week', 'is_weekend', 'is_holiday_season',
                       'flight_month', 'flight_year', 'search_month', 'search_day', 'days_to_flight_log',
                       'seatsRemaining', 'isRefundable'] + list(encoded_df.columns)
    # Add missing columns (those that are in the model but not in the chunk) with zeros
    if overall_batch_num > 0:
        # Ensure model has been trained first
        trained_feature_columns = xgb_model.get_booster().feature_names if hasattr(xgb_model, 'get_booster') else None
        if trained_feature_columns:
            for col in trained_feature_columns:
                if col not in encoded_df.columns:
                    encoded_df[col] = 0  # Add the missing column with all zeros
            # Ensure the columns are in the same order as the trained model
            encoded_df = encoded_df[trained_feature_columns]
    # Merge Encoded Data
    chunk_data = chunk_data.reset_index(drop=True)
    chunk_data = pd.concat([chunk_data, encoded_df], axis=1)

    X_chunk = chunk_data[feature_columns]
    y_chunk = chunk_data['totalFare']
    # Ensure the data is in the correct format
    X_chunk = X_chunk.values  # Convert to NumPy array
    y_chunk = y_chunk.values  # Convert to NumPy array
    # Include train-test split to create test set
    X_train, X_test, y_train, y_test = train_test_split(X_chunk, y_chunk, test_size=0.3, random_state=42)
    # Use DMatrix for better performance
    dtrain = DMatrix(X_train, label=y_train)
    dtest = DMatrix(X_test, label=y_test)

    total_batches = max(1, len(X_train) // 60000)  # Ensure at least 1 batch
    for batch_num in range(total_batches):
        start = batch_num * 60000
        end = min((batch_num + 1) * 60000, len(X_train))
        X_batch, y_batch = X_train[start:end], y_train[start:end]
        # Use overall_batch_num for conditional fit
        if overall_batch_num == 0:
            xgb_model.fit(X_batch, y_batch, verbose=False)
        else:
            xgb_model.fit(X_batch, y_batch, xgb_model=xgb_model.get_booster(), verbose=False)  # Continue training
        print(f":white_tick: Batch {batch_num + 1}/{total_batches}: Training on rows {start} to {end}...")
        # Increment overall batch number
        overall_batch_num += 1
    # Increment offset for the next chunk
    offset += chunk_size
    # :drawing_pin: Final Evaluation (After all chunks processed)
    print("\nTraining completed! Final evaluation on the most recent chunk...")
    mae_xgb = mean_absolute_error(y_train, xgb_model.predict(X_train))
    mse_xgb = mean_squared_error(y_train, xgb_model.predict(X_train))
    rmspe_xgb = rmspe(y_train, xgb_model.predict(X_train))
    nmse_xgb = nmse(y_train, xgb_model.predict(X_train))
    mape_xgb = (mae_xgb / y_train.mean()) * 100
    print(f"\n:bar_chart: XGBoost MAE on Train: ${mae_xgb:.2f}, MSE on Train: ${mse_xgb:.2f}, RMSPE: {rmspe_xgb:.2f}%, NMSE: {nmse_xgb:.4f}, MAPE: {mape_xgb:.2f}%")
    # Run MAE on test
    mae_xgb_test = mean_absolute_error(y_test, xgb_model.predict(X_test))
    mse_xgb_test = mean_squared_error(y_test, xgb_model.predict(X_test))
    rmspe_xgb_test = rmspe(y_test, xgb_model.predict(X_test))
    nmse_xgb_test = nmse(y_test, xgb_model.predict(X_test))
    mape_xgb_test = (mae_xgb_test / y_test.mean()) * 100
    print(f"\n:bar_chart: XGBoost MAE on Test: ${mae_xgb_test:.2f}, MSE on Test: ${mse_xgb_test:.2f}, RMSPE: {rmspe_xgb_test:.2f}%, NMSE: {nmse_xgb_test:.4f}, MAPE: {mape_xgb_test:.2f}%")
joblib.dump(xgb_model, 'final_regression_model_20mar')
print("Model saved successfully!")

# üß† Imports
import pandas as pd
import numpy as np
import joblib
import os
from google.cloud import bigquery
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import DMatrix, train as xgb_train

# üîê GCP Auth
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/travel-insider-452211-181bd2eba48e.json"

# üì° BigQuery Setup
PROJECT_ID = "travel-insider-452211"
DATASET_NAME = "travel_insider_dataset"
TABLE_NAME = "filtered_flights"
client = bigquery.Client()

# ‚öôÔ∏è XGBoost GPU Params
params = {
    "objective": "reg:squarederror",
    "tree_method": "hist",
    "learning_rate": 0.05,
    "max_depth": 8,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "random_state": 42,
    "device": "cuda"
}

# üß™ Metrics
def rmspe(y_true, y_pred):
    return np.sqrt(np.mean(((y_true - y_pred) / y_true) ** 2)) * 100
def nmse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred) / np.mean(y_true) ** 2

# üîÅ Chunk Parameters
chunk_size = 600000
offset = 0
total_rows = 16839840

# üíæ State Variables
encoder = None
feature_columns = None
model = None
all_X, all_y = [], []

# üîÅ Chunk Loop
while offset < total_rows:
    print(f"\n‚è≥ Querying rows {offset} to {offset + chunk_size}...")

    # üßæ Query BigQuery (explicit SELECT for safety)
    query = f"""
    SELECT legId, totalFare, searchDate, flightDate, startingAirport,
           destinationAirport, segmentsAirlineName, seatsRemaining, isRefundable
    FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`
    LIMIT {chunk_size} OFFSET {offset}
    """
    chunk_data = client.query(query).to_dataframe()

    # ‚úÖ Sanity checks
    print("üì¶ Columns received:", chunk_data.columns.tolist())
    if chunk_data.empty:
        print("‚ö†Ô∏è Empty chunk. Stopping.")
        break
    if 'totalFare' not in chunk_data.columns:
        raise ValueError("‚ùå 'totalFare' column is missing. Check schema.")

    # üßπ Clean NaNs
    chunk_data = chunk_data.dropna(subset=["totalFare"])
    if chunk_data.empty:
        print("‚ö†Ô∏è All rows dropped due to NaN in totalFare. Skipping.")
        offset += chunk_size
        continue

    # üß† Feature Engineering
    chunk_data["searchDate"] = pd.to_datetime(chunk_data["searchDate"])
    chunk_data["flightDate"] = pd.to_datetime(chunk_data["flightDate"])
    chunk_data["days_to_flight"] = (chunk_data["flightDate"] - chunk_data["searchDate"]).dt.days
    chunk_data = chunk_data[chunk_data["days_to_flight"] > 0]
    chunk_data["day_of_week"] = chunk_data["flightDate"].dt.dayofweek
    chunk_data["is_weekend"] = (chunk_data["day_of_week"] >= 5).astype(int)
    chunk_data["is_holiday_season"] = chunk_data["flightDate"].dt.month.isin([6, 7, 12]).astype(int)
    chunk_data["days_to_flight_squared"] = chunk_data["days_to_flight"] ** 2
    chunk_data["flight_month"] = chunk_data["flightDate"].dt.month
    chunk_data["search_month"] = chunk_data["searchDate"].dt.month
    chunk_data["search_day"] = chunk_data["searchDate"].dt.day
    chunk_data["days_to_flight_log"] = np.log1p(chunk_data["days_to_flight"])
    chunk_data["isRefundable"] = chunk_data["isRefundable"].map({"Yes": 1, "No": 0}).fillna(0).astype(int)

    # üè∑Ô∏è One-hot encode categorical vars (first chunk only fits)
    categorical = ["startingAirport", "destinationAirport", "segmentsAirlineName"]
    if encoder is None:
        encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
        encoded = encoder.fit_transform(chunk_data[categorical])
        joblib.dump(encoder, "encoder.joblib")
    else:
        encoded = encoder.transform(chunk_data[categorical])
    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())

    # üß© Assemble feature matrix
    chunk_data = pd.concat([chunk_data.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)
    if feature_columns is None:
        feature_columns = ["days_to_flight", "days_to_flight_squared", "day_of_week", "is_weekend", "is_holiday_season",
                           "flight_month", "search_month", "search_day", "days_to_flight_log",
                           "seatsRemaining", "isRefundable"] + list(encoded_df.columns)

    y_chunk = chunk_data["totalFare"].values
    chunk_data = chunk_data.reindex(columns=feature_columns, fill_value=0)
    print("üß™ Raw columns:", list(chunk_data.columns))
    print("üî¨ Column bytes:", [col.encode() for col in chunk_data.columns])
    X_chunk = chunk_data.values


    # üß™ Split for training
    X_train, X_test, y_train, y_test = train_test_split(X_chunk, y_chunk, test_size=0.3, random_state=42)
    dtrain = DMatrix(X_train, label=y_train)

    # üöÄ Train model incrementally
    model = xgb_train(params, dtrain, xgb_model=model, num_boost_round=50)

    print(f"‚úÖ Trained on rows {offset} to {offset + chunk_size}")
    all_X.append(X_chunk)
    all_y.append(y_chunk)

    offset += chunk_size

# üìà Final Evaluation
print("\nüìä Final Evaluation Starting...")
all_X = np.vstack(all_X)
all_y = np.hstack(all_y)

X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.3, random_state=42)
y_pred_train = model.predict(DMatrix(X_train))
y_pred_test = model.predict(DMatrix(X_test))

# üìè Metrics
print("\nüìâ Train Metrics")
print(f"MAE: ${mean_absolute_error(y_train, y_pred_train):.2f}")
print(f"MSE: ${mean_squared_error(y_train, y_pred_train):.2f}")
print(f"RMSPE: {rmspe(y_train, y_pred_train):.2f}%")
print(f"NMSE: {nmse(y_train, y_pred_train):.4f}")

print("\nüß™ Test Metrics")
print(f"MAE: ${mean_absolute_error(y_test, y_pred_test):.2f}")
print(f"MSE: ${mean_squared_error(y_test, y_pred_test):.2f}")
print(f"RMSPE: {rmspe(y_test, y_pred_test):.2f}%")
print(f"NMSE: {nmse(y_test, y_pred_test):.4f}")

# üíæ Save final model
joblib.dump(model, "final_regression_model.joblib")
print("‚úÖ Model saved to final_regression_model.joblib")

def mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mae_train = mean_absolute_error(y_train, y_pred_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmspe_train = rmspe(y_train, y_pred_train)
nmse_train = nmse(y_train, y_pred_train)
mape_train = mape(y_train, y_pred_train)

print(f"MAE: ${mae_train:.2f}")
print(f"MSE: ${mse_train:.2f}")
print(f"RMSPE: {rmspe_train:.2f}%")
print(f"NMSE: {nmse_train:.4f}")
print(f"MAPE: {mape_train:.2f}%")

mae_test = mean_absolute_error(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmspe_test = rmspe(y_test, y_pred_test)
nmse_test = nmse(y_test, y_pred_test)
mape_test = mape(y_test, y_pred_test)

print(f"MAE: ${mae_test:.2f}")
print(f"MSE: ${mse_test:.2f}")
print(f"RMSPE: {rmspe_test:.2f}%")
print(f"NMSE: {nmse_test:.4f}")
print(f"MAPE: {mape_test:.2f}%")

"""# New section"""